{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TT04: RAG Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "import sentence_transformers\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TT02 & TT03 Recap\n",
    "---\n",
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 757/757 [22:50<00:00,  1.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'document_writer': {'documents_written': 24212}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create document store\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "#indicate the types of files to expect\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\"])\n",
    "\n",
    "#we need converters for the types of documents we are getting\n",
    "text_file_converter = TextFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "\n",
    "#this will join all our documents so they can be fed through the pipeline together\n",
    "document_joiner = DocumentJoiner()\n",
    "\n",
    "#add cleaning and preprocessing functions\n",
    "document_cleaner = DocumentCleaner() #standardizes and removes extra whitespace\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50) #chunks the text with overlap for context, like we did with spaCy\n",
    "\n",
    "#create document embeddings\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\") #use prebuilt model\n",
    "document_embedder.warm_up() #download the embedding model\n",
    "\n",
    "#write the processed documents to the InMemboryDocumentStore\n",
    "document_writer = DocumentWriter(document_store)\n",
    "\n",
    "preprocessing_pipeline = Pipeline()\n",
    "preprocessing_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "preprocessing_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
    "preprocessing_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "preprocessing_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "preprocessing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
    "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# get all our documents\n",
    "file = \"./TT04_Document_Library/\"\n",
    "files = list(Path(file).rglob(\"*\"))\n",
    "\n",
    "# run the pipeline\n",
    "preprocessing_pipeline.run({\"file_type_router\": {\"sources\": files}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure OpenAI API Key is Set\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Chat Prompt Template\n",
    "template = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "You are an AI assistant specializing in emergency medical triage. \n",
    "Your task is to assess patient cases, determine the urgency level, and provide the best immediate recommendations.\n",
    "If you receive a question without a patient case you may answer or ask follow-up questions, but any query with a \n",
    "patient case should receive urgency level and immediate recommendations.\n",
    "\n",
    "Guidelines:\n",
    "- Red (Critical): Immediate life-threatening condition (e.g., heart attack, stroke).\n",
    "- Yellow (Urgent): Needs prompt medical attention but not critical (e.g., moderate breathing difficulty).\n",
    "- Green (Non-Urgent): Minor conditions (e.g., mild fever, minor injuries).\n",
    "\n",
    "### Contextual Information:\n",
    "{% for document in documents %}\n",
    "    - {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "### Patient Query:\n",
    "{{ question }}\n",
    "\n",
    "### Triage Decision & Rationale:\n",
    "\"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x00000268540DDE10>\n",
       "🚅 Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: OpenAIChatGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "🛤️ Connections\n",
       "  - retriever.documents -> document_joiner.documents (List[Document])\n",
       "  - document_joiner.documents -> prompt_builder.documents (List[Document])\n",
       "  - document_joiner.documents -> answer_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])\n",
       "  - llm.replies -> answer_builder.replies (List[ChatMessage])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Retriever\n",
    "retriever = InMemoryBM25Retriever(document_store, top_k=5)\n",
    "\n",
    "# Initialize Components\n",
    "prompt_builder = ChatPromptBuilder(template=template)\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "document_joiner = DocumentJoiner()\n",
    "answer_builder = AnswerBuilder()\n",
    "\n",
    "# initialize rag pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"document_joiner\", document_joiner)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", chat_generator)\n",
    "rag_pipeline.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "# Connect Components\n",
    "rag_pipeline.connect(\"retriever.documents\", \"document_joiner.documents\")\n",
    "rag_pipeline.connect(\"document_joiner.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")\n",
    "rag_pipeline.connect(\"document_joiner.documents\", \"answer_builder.documents\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TT04: Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Specialty:\n",
      "Emergency Room Reports\n",
      "\n",
      "Sample Name: Asthma in a 5-year-old\n",
      "\n",
      "Description: Mother states he has been wheezing and coughing.\n",
      "(Medical Transcription Sample Report)\n",
      "CHIEF COMPLAINT: This 5-year-old male presents to Children's Hospital Emergency Department by the mother with \"have asthma.\" Mother states he has been wheezing and coughing. They saw their primary medical doctor. He was evaluated at the clinic, given the breathing treatment and discharged home, was not having asthma, prescribed prednisone and an antibiotic. They told to go to the ER if he got worse. He has had some vomiting and some abdominal pain. His peak flows on the morning are normal at 150, but in the morning, they were down to 100 and subsequently decreased to 75 over the course of the day.\n",
      "\n",
      "PAST MEDICAL HISTORY: Asthma with his last admission in 07/2007. Also inclusive of frequent pneumonia by report.\n",
      "\n",
      "IMMUNIZATIONS: Up-to-date.\n",
      "\n",
      "ALLERGIES: Denied.\n",
      "\n",
      "MEDICATIONS: Advair, Nasonex, Xopenex, Zicam, Zithromax, prednisone, and albuterol.\n",
      "\n",
      "PAST SURGICAL HISTORY: Denied.\n",
      "\n",
      "SOCIAL HISTORY: Lives at home, here in the ED with the mother and there is no smoking in the home.\n",
      "\n",
      "FAMILY HISTORY: No noted exposures.\n",
      "\n",
      "REVIEW OF SYSTEMS: Documented on the template. Systems reviewed on the template.\n",
      "\n",
      "PHYSICAL EXAMINATION:\n",
      "VITAL SIGNS: Temperature 98.7, pulse 105, respiration is 28, blood pressure 112/65, and weight of 16.5 kg. Oxygen saturation low at 91% on room air.\n",
      "\n",
      "GENERAL: This is a well-developed male who is cooperative, alert, active with oxygen by facemask.\n",
      "\n",
      "HEENT: Head is atraumatic and normocephalic. Pupils are equal, round, and reactive to light. Extraocular motions are intact and conjugate. Clear TMs, nose, and oropharynx.\n",
      "\n",
      "NECK: Supple. Full painless nontender range of motion.\n",
      "\n",
      "CHEST: Tight wheezing and retractions heard bilaterally.\n",
      "\n",
      "HEART: Regular without rubs or murmurs.\n",
      "\n",
      "ABDOMEN: Soft, nontender. No masses. No hepatosplenomegaly.\n",
      "\n",
      "GENITALIA: Male genitalia is present on a visual examination.\n",
      "\n",
      "SKIN: No significant bruising, lesions or rash.\n",
      "\n",
      "EXTREMITIES: Moves all extremities without difficulty, nontender. No deformity.\n",
      "\n",
      "NEUROLOGIC: Symmetric face, cooperative, and age appropriate.\n",
      "\n",
      "MEDICAL DECISION MAKING: He is evaluated in the emergency department with continuous high-dose albuterol, Decadron by mouth, pulse oximetry, and close observation. Chest x-ray reveals bronchial thickening, otherwise no definite infiltrate. He is further treated in the emergency department with continued breathing treatments. He has continued tight wheezes with saturations 99%, but ED sats are 92% with coughing spells.\n"
     ]
    }
   ],
   "source": [
    "with open(\"medical_case_1.txt\", \"r\") as f:\n",
    "    query = f.read()\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline\n",
    "response = rag_pipeline.run(\n",
    "            {\n",
    "                \"retriever\": {\"query\": query},\n",
    "                \"prompt_builder\": {\"question\": query},\n",
    "                \"answer_builder\": {\"query\": query},\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "# Extract the generated answer\n",
    "answer_text = response[\"answer_builder\"][\"answers\"][0].data\n",
    "\n",
    "# Extract original query\n",
    "query = response[\"answer_builder\"][\"answers\"][0].query\n",
    "\n",
    "# Extract retrieved document texts\n",
    "retrieved_docs = response[\"answer_builder\"][\"answers\"][0].documents\n",
    "document_texts = [doc.content for doc in retrieved_docs]\n",
    "\n",
    "# set up the evaluation dataset\n",
    "eval_dict = {\n",
    "    \"user_input\": query,  # User's original query\n",
    "    \"response\": answer_text,  # AI-generated answer\n",
    "    \"retrieved_contexts\": document_texts  # Retrieved documents\n",
    "}\n",
    "\n",
    "# method expects a list of dicts\n",
    "evaluation_dataset = EvaluationDataset.from_dict([eval_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**AI Triage Recommendation:**\n",
      "\n",
      "**Urgency Level: Yellow (Urgent)**\n",
      "\n",
      "**Rationale:** The patient is a 5-year-old boy with a history of asthma showing wheezing, coughing, signs of respiratory distress, and low oxygen saturation (91% on room air). Although he is alert and cooperating, he has had a significant drop in peak flow and now has continued audible wheezing, which indicates that his asthma is not well controlled. His vital signs (respiratory rate of 28 and heart rate of 105) indicate some level of distress, and he is showing signs of respiratory compromise with low oxygen saturation.\n",
      "\n",
      "**Immediate Recommendations:**\n",
      "1. **Administer Supplemental Oxygen:** Increase oxygen delivery to improve saturation immediately while continuing monitoring.\n",
      "  \n",
      "2. **Continue High-Dose Albuterol Therapy:** Ensure the child receives continuous nebulizer treatments to manage bronchospasm.\n",
      "\n",
      "3. **Administer Corticosteroids:** Decadron (dexamethasone) should continue as it helps reduce airway inflammation.\n",
      "\n",
      "4. **Monitor Vital Signs:** Regularly check pulse oximetry and work of breathing during treatment.\n",
      "\n",
      "5. **Consider Further Interventions if No Improvement:** If the child's condition does not improve or worsens (i.e., if oxygen saturation remains low or respiratory distress increases), be prepared to escalate treatment (consideration of magnesium sulfate, subcutaneous terbutaline, or IV fluids as needed).\n",
      "\n",
      "6. **Consult Pediatric Pulmonology or Critical Care:** Potential admission for observation and ongoing management might be necessary depending on the response to treatment in the ED.\n",
      "\n",
      "The child should be closely monitored for any signs of deterioration, and further interventions may be necessary depending on ongoing assessments.\n",
      "\n",
      "**Retrieved Documents:**\n",
      "\n",
      "Document 1 Excerpt: The primary toxicity from hydrocar- bon is from as...\n",
      "\n",
      "Document 2 Excerpt: therapy. 3. A 27-year-old man presents to the emer...\n",
      "\n",
      "Document 3 Excerpt: after dinner. His vital signs are normal. While co...\n",
      "\n",
      "Document 4 Excerpt: severe acidosis, unreactive pupils, GCS of 3, and ...\n",
      "\n",
      "Document 5 Excerpt: underwent a successful percutaneous coronary inter...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the AI response\n",
    "print(\"**AI Triage Recommendation:**\\n\")\n",
    "print(answer_text)\n",
    "\n",
    "# Print retrieved document excerpts\n",
    "print(\"\\n**Retrieved Documents:**\\n\")\n",
    "for i, doc in enumerate(document_texts):\n",
    "    print(f\"Document {i+1} Excerpt: {doc[:50]}...\\n\")  # Show first 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emtay\\AppData\\Local\\Temp\\ipykernel_660\\1648821255.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) #temperature = 0 for more consistent results\n",
      "Evaluating: 100%|██████████| 4/4 [00:26<00:00,  6.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.8609, 'patient triaged': 1.0000, 'faithfulness': 0.4783, 'llm_context_precision_without_reference': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import AnswerRelevancy, Faithfulness, LLMContextPrecisionWithoutReference, AspectCritic\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM Wrapper\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) #temperature = 0 for more consistent results\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Run Ragas Evaluation\n",
    "ragas_evaluator = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[AnswerRelevancy(), \n",
    "             AspectCritic(name=\"patient triaged\",\n",
    "                          definition=\"Does the response include a triage level of Red, Yellow, or Green?\",\n",
    "                          llm=evaluator_llm), \n",
    "             Faithfulness(llm=evaluator_llm), \n",
    "             LLMContextPrecisionWithoutReference(llm=evaluator_llm)]\n",
    ")\n",
    "\n",
    "print(ragas_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "- Haystack - Ragas. (2025). Ragas.io. https://docs.ragas.io/en/stable/howtos/integrations/haystack/\n",
    "- RagasEvaluator. (2025). Haystack Documentation. https://docs.haystack.deepset.ai/docs/ragasevaluator\n",
    "- RAG Pipeline Evaluation Using RAGAS | Haystack. (2024). Haystack. https://haystack.deepset.ai/cookbook/rag_eval_ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
